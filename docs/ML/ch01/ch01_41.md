# [ML] Classic Regressor (Summary)

DeepLearning 계열을 제외한 Regressor 모델들을 간단하게 정리함.
​

* [관련 ipynb파일](https://gist.github.com/dsaint31x/1c9c4a27e1d841098a9fee345363fa59)
​
---
​
## **분류**
​
* Instance Based Algorithm
    -  K Neighbors Regression
* Model Based Algorithm
    - Linear Model
        - Linear Regression
        - Regularization
            - Ridge Regression
            - Lasso Regression
            - ElasticNet
    - Non-Linear Model
        - Polynomial Regression
        - Support Vector Regression
        - Decision Tree Regression
        - [Ensemble Model](https://dsaint31.tistory.com/798)
            - Random Forest Regression
            - AdaBoost Regression
            - Gradient Booting Regression
            - XGBoost, LightGBM, CatBoost

​
---
​
## **설명**
​
### **0\. K-Nearest Neighbors Regressor (KNN 회귀)**
​
-  **설명**:
    - 가장 가까운 K개의 데이터 포인트를 기반으로 예측하는 instance based model (1951)
- **사용 함수**:
-   - `sklearn.neighbors.KNeighborsRegressor`
- **특징**:
    - 비선형 데이터에 강하지만, K 값에 따라 성능 차이.
​
```Python
# KNN Regressor 모델 (기본값으로 k=5 사용)
from sklearn.neighbors import KNeighborsRegressor
​
# KNeighborsRegressor 모델 생성
knn_regressor = KNeighborsRegressor(
    n_neighbors=5,             # 이웃 수 (k 값)
    weights='distance',        # 거리 가중치 ('uniform' 또는 'distance')
    algorithm='auto',          # 가까운 이웃을 찾는 알고리즘 ('auto', 'ball_tree', 'kd_tree', 'brute')
    leaf_size=30,              # 트리 기반 알고리즘에서의 leaf 사이즈
    p=2,                       # 거리 측정 방법 (1: 맨해튼 거리, 2: 유클리드 거리)
    metric='minkowski',        # 거리 계산에 사용할 메트릭 ('minkowski', 'euclidean', 'manhattan' 등)
    n_jobs=-1                  # 병렬처리할 CPU 코어 수 (-1은 모든 코어 사용)
)
knn_regressor = knn_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **1\. Linear Regression (선형 회귀)**

- **설명**: 데이터의 features과 target 간의 linear relation를 가정. (1800s)
- **사용 함수**: `sklearn.linear_model.LinearRegression`
- **특징**: 가장 간단한 회귀 모델, under-fitting 우려.
​
```Python
from sklearn.linear_model import LinearRegression
​
# LinearRegression 모델 생성
linear_regressor = LinearRegression(
    fit_intercept=True,     # 절편을 계산할지 여부 (True: 절편을 계산, False: 절편 없이 원점을 지남)
    copy_X=True,            # 입력 데이터 X의 복사본을 만들지 여부 (True: 원본을 유지)
    n_jobs=None,            # 사용할 CPU 코어 수 (-1: 모든 코어 사용, None: 자동)
    positive=False          # 계수가 양수로 제약되는지 여부 (True: 모든 계수가 양수여야 함)
)
linear_regressor = linear_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **2\. Ridge Regression (릿지 회귀)**
​
- **설명**: 선형 회귀에 L2 Regularization 를 추가하여 over-fitting 방지. (1970s)
- **사용 함수**: `sklearn.linear_model.Ridge`
- **특징**: 큰 Parameters를 같이 작은 수를 취하도록 regularization → 모델의 자유도를 떨어뜨려 generalization.
​
```Python
from sklearn.linear_model import Ridge
​
# Ridge 회귀 모델 생성
ridge_regressor = Ridge(
    alpha=1.0,          # 정규화 강도 (alpha 값이 클수록 정규화가 강해짐)
    fit_intercept=True, # 절편을 계산할지 여부
    copy_X=True,        # 원본 데이터를 복사할지 여부
    max_iter=None,      # 최대 반복 횟수 (default는 무제한)
    tol=0.001,          # 수렴 기준 허용 오차
    solver='auto'       # 계산 방법 선택 ('auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg' 등)
)
ridge_regressor = ridge_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **3\. Lasso Regression (라쏘 회귀)**
​
-   **설명**: 선형 회귀에 L1 Regularization를 추가하여 over-fitting 방지. (1996)
-   **사용 함수**: `sklearn.linear_model.Lasso`
-   **특징**: 불필요한 features와 관련된 parameters를 0으로 만들어서 sparse weight matrix가 되도록 함.
​
```Python
from sklearn.linear_model import Lasso
​
# Lasso 회귀 모델 생성
lasso_regressor = Lasso(
    alpha=1.0,          # 정규화 강도 (alpha 값이 클수록 정규화가 강해짐)
    fit_intercept=True, # 절편을 계산할지 여부
    precompute=False,   # 계산 속도 향상을 위한 미리 계산된 행렬 사용 여부
    max_iter=1000,      # 최대 반복 횟수
    tol=0.0001,         # 수렴 기준 허용 오차
    positive=False,     # 계수를 양수로 제한할지 여부
    selection='cyclic'  # 계수 업데이트 순서 ('cyclic' 또는 'random')
)
​
lasso_regressor = lasso_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **4\. ElasticNet**

- **설명**: Lasso와 Ridge를 혼합한 모델. (2005)
- **사용 함수**: `sklearn.linear_model.ElasticNet`
- **특징**: L1과 L2 Regularization 의 장점을 동시에 사용.
​
```Python
from sklearn.linear_model import ElasticNet
​
# ElasticNet 회귀 모델 생성
elasticnet_regressor = ElasticNet(
    alpha=1.0,          # 전체 정규화 강도 (alpha 값이 클수록 정규화가 강해짐)
    l1_ratio=0.5,       # L1 비율 (0.5는 L1과 L2를 동일하게 사용)
    fit_intercept=True, # 절편을 계산할지 여부
    precompute=False,   # 계산 속도 향상을 위한 미리 계산된 행렬 사용 여부
    max_iter=1000,      # 최대 반복 횟수
    tol=0.0001,         # 수렴 기준 허용 오차
    positive=False,     # 계수를 양수로 제한할지 여부
    selection='cyclic'  # 계수 업데이트 순서 ('cyclic' 또는 'random')
)
​
elasticnet_regressor = elasticnet_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **5\. Polynomial Regression (다항 회귀)**
​
* **설명**: 입력 features를 polynomial equation으로 변경하여 non-linear relation을 modeling.
* **사용 함수**: `sklearn.preprocessing.PolynomialFeatures` + `LinearRegression`
* **특징**: 다항식 degree(차수)에 따라 복잡도가 증가.
​

```Python
from sklearn.preprocessing import PolynomialFeatures
​
# PolynomialFeatures로 다항식 특성 생성
poly_features = PolynomialFeatures(degree=2, include_bias=True)
X_train_poly = poly_features.fit_transform(X_train_scaled)
X_test_poly = poly_features.transform(X_test_scaled)
​
from sklearn.linear_model import ElasticNet
​
# ElasticNet 회귀 모델 생성
elasticnet_regressor_nl = ElasticNet(
    alpha=1.0,          # 전체 정규화 강도 (alpha 값이 클수록 정규화가 강해짐)
    l1_ratio=0.5,       # L1 비율 (0.5는 L1과 L2를 동일하게 사용)
    fit_intercept=True, # 절편을 계산할지 여부
    precompute=False,   # 계산 속도 향상을 위한 미리 계산된 행렬 사용 여부
    max_iter=1000,      # 최대 반복 횟수
    tol=0.0001,         # 수렴 기준 허용 오차
    positive=False,     # 계수를 양수로 제한할지 여부
    selection='cyclic'  # 계수 업데이트 순서 ('cyclic' 또는 'random')
)
​
elasticnet_regressor_nl = elasticnet_regressor_nl.fit(X_train_poly, y_train)
```
​
---
​
### **6\. Support Vector Regression (SVR)**
​
* **설명**: 데이터의 margine 을 최대화하며 회귀를 수행. (1996)
* **사용 함수**: `sklearn.svm.SVR`
* **특징**: 비선형 경계를 modeling 할 수 있음. data samples의 수가 너무 많을 시 성능이 감소하기 쉬임.
​
```Python
from sklearn.svm import SVR
​
# SVR 모델 생성
svr_regressor = SVR(
    kernel='rbf',        # 커널 유형: 'linear', 'poly', 'rbf', 'sigmoid'
    degree=3,            # 다항식 커널을 사용할 때, 다항식의 차수 (기본값 3)
    gamma='scale',       # 커널 계수: 'scale', 'auto' 또는 float 값 (RBF, poly, sigmoid에서 사용)
    coef0=0.0,           # 다항식 및 시그모이드 커널에서 상수 항
    tol=1e-3,            # 반복 과정에서 수렴 기준 허용 오차
    C=3.0,               # 규제 강도: 값이 클수록 오차를 줄이려 함 (과적합 위험): 1.0
    epsilon=0.01,        # 예측 정확도에 허용할 오차의 마진
    shrinking=True,      # 축소 방식 사용 여부 (True: 최적화 시 더 빠를 수 있음)
    cache_size=200,      # SVM에서 커널 계산을 위한 캐시 크기 (MB 단위)
    verbose=False,       # 학습 과정에서 출력 여부
    max_iter=-1          # 최대 반복 횟수 (-1이면 무제한)
)
​
svr_regressor = svr_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **7\. Decision Tree Regressor (결정 트리 회귀)**
​
* **설명**: 데이터 공간을 Tree 형태로 분할하여 예측. (1980s)
* **사용 함수**: `sklearn.tree.DecisionTreeRegressor`
* **특징**: 비선형 관계를 잘 모델링하고 feature scaling이 필요없음. over-fitting 가능성이 매우 높아서 Random Forest Regressor가 사용됨(Random Forest Regressor를 이해하기 위한 stepping stone).
​
```Python
from sklearn.tree import DecisionTreeRegressor
​
# DecisionTreeRegressor 모델 생성
tree_regressor = DecisionTreeRegressor(
    criterion='squared_error',   # 분할 품질 기준 ('squared_error', 'friedman_mse', 'absolute_error', 'poisson')
    splitter='best',             # 분할 방법 ('best': 가장 좋은 분할 선택, 'random': 랜덤 분할)
    max_depth=None,              # 트리의 최대 깊이 (None이면 깊이에 제한이 없음)
    min_samples_split=2,         # 내부 노드를 분할하기 위한 최소 샘플 수
    min_samples_leaf=1,          # 리프 노드에 있어야 할 최소 샘플 수
    min_weight_fraction_leaf=0.0,# 리프 노드에 필요한 가중치 합의 최소 비율
    max_features=None,           # 분할에 사용할 최대 특성 수 (None이면 모든 특성 사용)
    max_leaf_nodes=None,         # 리프 노드의 최대 개수 (None이면 제한 없음)
    min_impurity_decrease=0.0,   # 노드를 분할하는 데 필요한 최소 불순도 감소
    ccp_alpha=0.0,               # 가지치기 비용-복잡도 매개변수 (0.0이면 가지치기 없음)
    random_state=7,           # 트리의 무작위성을 제어하는 시드 값
)
​
tree_regressor = tree_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **8\. Random Forest Regressor (랜덤 포레스트 회귀)**
​
* **설명**: 여러 개의 Decision Tree (=weak learner)를 ensemble하여 예측 (aggregation: averaging)하는 Bagging 기법. (2001)
* **사용 함수**: `sklearn.ensemble.RandomForestRegressor`
* **특징**: over-fitting 방지 및 generalization 성능 향상.
​

```Python
from sklearn.ensemble import RandomForestRegressor
​
# RandomForestRegressor 모델 생성
forest_regressor = RandomForestRegressor(
    n_estimators=100,            # 생성할 트리의 개수 (기본값: 100)
    criterion='squared_error',   # 분할 기준 ('squared_error': MSE, 'absolute_error': MAE) *
    max_depth=None,              # 트리의 최대 깊이 (None이면 제한 없음) *
    min_samples_split=2,         # 노드를 분할하기 위한 최소 샘플 수 *
    min_samples_leaf=1,          # 리프 노드에 있어야 하는 최소 샘플 수 *
    min_weight_fraction_leaf=0.0,# 리프 노드에서 최소 가중치 비율 *
    max_features='sqrt',         # 각 분할에 사용할 최대 특성 수 ('sqrt', 'log2' 또는 정수) * 
    max_leaf_nodes=None,         # 최대 리프 노드 수 (None이면 제한 없음) *
    min_impurity_decrease=0.0,   # 분할을 위한 최소 불순도 감소 * 
    random_state=7,           # 무작위성 제어 (재현 가능성을 위한 시드)
    bootstrap=True,              # 부트스트랩 샘플링 사용 여부
    oob_score=False,             # OOB (Out of Bag) 샘플로 성능 평가 여부
    n_jobs=None,                 # 병렬 처리에 사용할 CPU 코어 수 (-1이면 모든 코어 사용)
    verbose=0,                   # 학습 과정 중 출력 여부
    warm_start=False,            # 이전 학습 결과를 이용해 추가 학습을 할지 여부
    ccp_alpha=0.0,               # 비용-복잡도 가지치기의 매개변수 (0이면 가지치기 없음)
    max_samples=None             # 부트스트랩 샘플링 시 사용할 최대 샘플 비율
)
​
forest_regressor = forest_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **9\. AdaBoost Regressor (아다부스트 회귀)**
​
-   **설명**: 약한 학습자(보통 결정 트리)를 결합하여 성능을 향상시키는 Boosting 기법. (1995)
-   **사용 함수**: sklearn.ensemble.AdaBoostRegressor
-   **특징**: 점진적으로 학습하며, 이전 단계에서 잘못 예측한 데이터에 더 집중.
​
```Python
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
​
# AdaBoostRegressor 모델 생성
​
adaboost_regressor = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=3),  # 약한 학습자로 결정 트리 회귀 사용
    n_estimators=50,             # 부스팅 과정에서 생성할 트리(약한 학습자)의 개수
    learning_rate=1.0,           # 학습률: 각 학습자가 기여하는 정도를 제어
    loss='linear',               # 손실 함수 ('linear', 'square', 'exponential')
    random_state=42              # 무작위성을 제어하는 시드 값
)
adaboost_regressor = adaboost_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **10\. Gradient Boosting Regressor (그래디언트 부스팅 회귀)**
​
* **설명**: weak learner를 직렬로 결합하여 점진적으로 오류를 줄이는 ensemble 기법: Boosting 에 속함. (1999)
* **사용 함수**: `sklearn.ensemble.GradientBoostingRegressor`
* **특징**: 강력한 성능, 학습이 느릴 수 있으며 hyper-parameter에 의한 성능 차이가 심한 편이며 over-fitting의 위험이 존재.


```Python
from sklearn.ensemble import GradientBoostingRegressor
​
# GradientBoostingRegressor 모델 생성
gbr_regressor = GradientBoostingRegressor(
    loss='squared_error',        # 손실 함수 ('squared_error', 'absolute_error', 'huber', 'quantile')
    learning_rate=0.01,          # 학습률, 각 트리가 기여하는 정도를 제어: 0.1
    n_estimators=100,            # 생성할 트리의 개수
    subsample=1.0,               # 각 트리 학습에 사용할 샘플 비율 (1.0이면 모든 샘플 사용)
    criterion='friedman_mse',    # 트리 분할 기준 ('friedman_mse', 'squared_error', 'mse')
    min_samples_split=2,         # 내부 노드를 분할하기 위한 최소 샘플 수
    min_samples_leaf=1,          # 리프 노드에 있어야 하는 최소 샘플 수
    min_weight_fraction_leaf=0.0,# 리프 노드에서의 가중치 최소 비율
    max_depth=3,                 # 트리의 최대 깊이
    min_impurity_decrease=0.0,   # 노드를 분할하는 데 필요한 불순도 감소
    init=None,                   # 초기 추정기 (None이면 평균값으로 시작)
    random_state=7,             # 랜덤 시드 값
    max_features=None,           # 각 분할에서 고려할 최대 특성 수
    alpha=0.9,                   # 허브 또는 분위수 손실 함수를 사용할 때 분위수 값
    verbose=0,                   # 학습 과정 중 출력 여부
    max_leaf_nodes=None,         # 리프 노드의 최대 개수
    warm_start=False,            # True로 설정하면 이전 학습 결과에서 추가 학습 가능
    validation_fraction=0.1,     # 조기 종료를 위해 검증용으로 사용할 훈련 데이터 비율
    n_iter_no_change=None,       # 조기 종료를 위해 반복할 동안 성능 향상이 없는 경우의 허용치
    tol=1e-4,                    # 성능 향상이 없을 때 종료하기 위한 허용 오차
    ccp_alpha=0.0                # 비용-복잡도 가지치기의 매개변수 (0.0이면 가지치기 없음)
)
​
gbr_regressor = gbr_regressor.fit(X_train_scaled, y_train)
```
​
---
​
### **11\. XGBoost / LightGBM / CatBoost (외부 라이브러리 사용)**
​
* **설명**: Gradient Boosting을 더 최적화한 고성능 모델들.
* **사용 함수**:
    - `xgboost.XGBRegressor`: GB를 최적화 (2014)
    - `lightgbm.LGBMRegressor`: 빠른 learning 과 낮은 메모리 사용을 목표로 개발 (2017)
    - `catboost.CatBoostRegressor`: Categorical data를 효율적으로 처리하는 GB (2017)
* **특징**: 복잡한 데이터셋에서도 우수한 성능을 보이나 third-party library가 필요.
​
---
​
XGBRegressor


```Python
import xgboost as xgb
​
# XGBRegressor 모델 생성
xgb_regressor = xgb.XGBRegressor(
    objective='reg:squarederror',  # 목적 함수 (회귀: 'reg:squarederror', 'reg:squaredlogerror' 등)
    n_estimators=100,              # 부스팅 과정에서 생성할 트리의 개수
    learning_rate=0.1,             # 학습률 (각 트리의 기여도를 제어)
    max_depth=3,                   # 각 트리의 최대 깊이
    min_child_weight=1,            # 리프 노드가 가져야 할 최소 가중치 합
    gamma=0,                       # 노드를 분할하기 위한 최소 손실 감소
    subsample=1.0,                 # 각 트리 학습에 사용할 데이터 샘플의 비율
    colsample_bytree=1.0,          # 각 트리 학습에 사용할 특성의 비율
    reg_alpha=0,                   # L1 정규화 항의 가중치 (Lasso)
    reg_lambda=1,                  # L2 정규화 항의 가중치 (Ridge)
    booster='gbtree',              # 부스팅 알고리즘 ('gbtree', 'gblinear', 'dart')
    n_jobs=1,                      # 병렬 처리에 사용할 CPU 코어 수
    random_state=7,               # 무작위성을 제어하기 위한 시드 값
    verbosity=1                    # 출력 메시지 수준 (0: 출력 없음, 1: 경고, 2: 정보)
)
​
xgb_regressor = xgb_regressor.fit(X_train_scaled, y_train)
```
​
---
​
LGBMRegressor

​
```Python
import lightgbm as lgb
​
# LGBMRegressor 모델 생성
lgb_regressor = lgb.LGBMRegressor(
    boosting_type='gbdt',        # 부스팅 유형 ('gbdt', 'rf', 'dart', 'goss')
    num_leaves=20,               # 하나의 트리가 가질 수 있는 최대 리프 노드 수:31
    max_depth=5,                # 트리의 최대 깊이 (-1은 제한 없음): -1
    learning_rate=0.1,           # 학습률
    n_estimators=100,            # 부스팅 과정에서 생성할 트리의 개수
    subsample_for_bin=200000,    # 각 feature에 대해 데이터 포인트를 사용할 최대 bin 개수
    objective='regression',      # 목적 함수 (회귀 문제)
    class_weight=None,           # 클래스 가중치 (None이면 균등한 가중치 사용)
    min_split_gain=0.0,          # 노드를 분할할 때 필요한 최소 손실 감소
    min_child_weight=1e-3,       # 리프 노드가 가져야 할 최소 가중치 합
    min_child_samples=20,        # 리프 노드에 있어야 할 최소 샘플 수
    subsample=1.0,               # 각 트리 학습에 사용할 데이터 샘플 비율
    subsample_freq=0,            # Subsample 수행 빈도
    colsample_bytree=1.0,        # 각 트리 학습에 사용할 특성의 비율
    reg_alpha=0.0,               # L1 정규화 계수
    reg_lambda=0.0,              # L2 정규화 계수
    random_state=7,           # 무작위성 제어를 위한 시드 값
    n_jobs = -1,                    # 병렬 처리에 사용할 CPU 코어 수 (-1이면 모든 코어 사용)
    verbose = -1, # -1: 모든 로그 메시지를 끔. 0: 정보 로그를 숨기고 경고 메시지만 표시/ 1: 기본 설정으로 모든 로그 메시지를 표시. 
)
​
lgb_regressor = lgb_regressor.fit(X_train_scaled, y_train)
```
​
---
​
CatBoostRegressor

​
```Python
# !pip install catboost
from catboost import CatBoostRegressor
​
# CatBoostRegressor 모델 생성
catboost_regressor = CatBoostRegressor(
    iterations=1000,           # 부스팅 반복 횟수 (생성할 트리의 개수)
    learning_rate=0.1,         # 학습률
    depth=6,                   # 트리의 최대 깊이
    loss_function='RMSE',      # 손실 함수 (기본적으로 RMSE 사용)
    eval_metric='RMSE',        # 평가 메트릭 (회귀 문제에서는 RMSE 또는 MAE 사용 가능)
    random_seed=42,            # 무작위성 제어를 위한 시드 값
    l2_leaf_reg=3.0,           # L2 정규화 계수
    bootstrap_type='Bayesian', # 부트스트랩 샘플링 방식 ('Bayesian', 'Bernoulli', 'MVS' 등)
    # subsample=0.8,             # 각 트리 학습에 사용할 데이터 샘플 비율
    task_type='CPU',           # 'GPU'를 사용할 경우 GPU로 학습 (기본값은 'CPU')
    verbose=100                # 학습 과정에서 출력 메시지 빈도 (0으로 설정 시 로그 출력 비활성화)
)
​
catboost_regressor = catboost_regressor.fit(X_train_scaled, y_train)
```
